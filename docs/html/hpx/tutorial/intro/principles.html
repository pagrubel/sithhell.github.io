<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<title>Governing Principles applied while Developing HPX</title>
<link rel="stylesheet" href="../../../src/boostbook.css" type="text/css">
<meta name="generator" content="DocBook XSL Stylesheets V1.78.1">
<link rel="home" href="../../../index.html" title="HPX 0.9.9">
<link rel="up" href="../intro.html" title="Introduction">
<link rel="prev" href="new_response.html" title="Technology Demands New Response">
<link rel="next" href="../examples.html" title="Examples">
</head>
<body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF">
<table cellpadding="2" width="100%"><tr><td valign="top"><img alt="HPX - High Performance ParalleX" width="440" height="104" src="../../../images/hpx_0_9_9_draft.png"></td></tr></table>
<hr>
<div class="spirit-nav">
<a accesskey="p" href="new_response.html"><img src="../../../images/prev.png" alt="Prev"></a><a accesskey="u" href="../intro.html"><img src="../../../images/up.png" alt="Up"></a><a accesskey="h" href="../../../index.html"><img src="../../../images/home.png" alt="Home"></a><a accesskey="n" href="../examples.html"><img src="../../../images/next.png" alt="Next"></a>
</div>
<div class="section">
<div class="titlepage"><div><div><h4 class="title">
<a name="hpx.tutorial.intro.principles"></a><a class="link" href="principles.html" title="Governing Principles applied while Developing HPX">Governing Principles
        applied while Developing <span class="emphasis"><em>HPX</em></span></a>
</h4></div></div></div>
<p>
          As it turn out, we do not have to start from scratch. Not everything has
          to be invented and designed anew. Many of the ideas needed to combat the
          4 horsemen have already been had, often more than 30 years ago. All it
          takes is to gather them into a coherent approach. So please let me highlight
          some of the derived principles we think to be crucial for defeating <span class="bold"><strong>SLOW</strong></span>. Some of those are focused on high-performance
          computing, others are more general.
        </p>
<h6>
<a name="hpx.tutorial.intro.principles.h0"></a>
          <span class="phrase"><a name="hpx.tutorial.intro.principles.latency_hiding"></a></span><a class="link" href="principles.html#hpx.tutorial.intro.principles.latency_hiding">Focus
          on Latency Hiding instead of Latency Avoidance</a>
        </h6>
<p>
          It is impossible to design a system exposing zero latencies. In an effort
          to come as close as possible to this goal many optimizations are mainly
          targeted towards minimizing latencies. Examples for this can be seen everywhere,
          for instance low latency network technologies like <a href="http://en.wikipedia.org/wiki/InfiniBand" target="_top">InfiniBand</a>,
          caching memory hierarchies in all modern processors, the constant optimization
          of existing <a href="http://en.wikipedia.org/wiki/Message_Passing_Interface" target="_top">MPI</a>
          implementations to reduce related latencies, or the data transfer latencies
          intrinsic to the way we use <a href="http://en.wikipedia.org/wiki/GPGPU" target="_top">GPGPUs</a>
          today. It is important to note, that existing latencies are often tightly
          related to some resource having to wait for the operation to be completed.
          At the same time it would be perfectly fine to do some other, unrelated
          work in the meantime, allowing to hide the latencies by filling the idle-time
          with useful work. Modern system already employ similar techniques (pipelined
          instruction execution in the processor cores, asynchronous input/output
          operations, and many more). What we propose is to go beyond anything we
          know today and to make latency hiding an intrinsic concept of the operation
          of the whole system stack.
        </p>
<h6>
<a name="hpx.tutorial.intro.principles.h1"></a>
          <span class="phrase"><a name="hpx.tutorial.intro.principles.parallelism"></a></span><a class="link" href="principles.html#hpx.tutorial.intro.principles.parallelism">Embrace
          Fine-grained Parallelism instead of Heavyweight Threads</a>
        </h6>
<p>
          If we plan to hide latencies even for very short operations, such as fetching
          the contents of a memory cell from main memory (if it is not already cached),
          we need to have very lightweight threads with extremely short context switching
          times, optimally executable within one cycle. Granted, for mainstream architectures
          this is not possible today (even if we already have special machines supporting
          this mode of operation, such as the <a href="http://en.wikipedia.org/wiki/Cray_XMT" target="_top">Cray
          XMT</a>). For conventional systems however, the smaller the overhead
          of a context switch and the finer the granularity of the threading system,
          the better will be the overall system utilization and its efficiency. For
          today's architectures we already see a flurry of libraries providing exactly
          this type of functionality: non-preemptive, task-queue based parallelization
          solutions, such as <a href="http://threadingbuildingblocks.org/" target="_top">Intel
          Threading Building Blocks (TBB)</a>, <a href="http://msdn.microsoft.com/en-us/library/dd492418.aspx" target="_top">Microsoft
          Parallel Patterns Library (PPL)</a>, <a href="http://software.intel.com/en-us/articles/intel-cilk-plus/" target="_top">Cilk++</a>,
          and many others. The possibility to suspend a current task if some preconditions
          for its execution are not met (such as waiting for I/O or the result of
          a different task), seamlessly switching to any other task which can continue,
          and to reschedule the initial task after the required result has been calculated,
          which makes the implementation of latency hiding almost trivial.
        </p>
<h6>
<a name="hpx.tutorial.intro.principles.h2"></a>
          <span class="phrase"><a name="hpx.tutorial.intro.principles.synchronization"></a></span><a class="link" href="principles.html#hpx.tutorial.intro.principles.synchronization">Rediscover
          Constrained Based Synchronization to replace Global Barriers</a>
        </h6>
<p>
          The code we write today is riddled with implicit (and explicit) global
          barriers. When I say global barrier I mean the synchronization of the control
          flow between several (very often all) threads (when using <a href="http://openmp.org/wp/" target="_top">OpenMP</a>)
          or processes (<a href="http://en.wikipedia.org/wiki/Message_Passing_Interface" target="_top">MPI</a>).
          For instance, an implicit global barrier is inserted after each loop parallelized
          using <a href="http://openmp.org/wp/" target="_top">OpenMP</a> as the system synchronizes
          the threads used to execute the different iterations in parallel. In <a href="http://en.wikipedia.org/wiki/Message_Passing_Interface" target="_top">MPI</a>
          each of the communication steps imposes an explicit barrier onto the execution
          flow as (often all) nodes have be synchronized. Each of those barriers
          acts as an eye of the needle the overall execution is forced to be squeezed
          through. Even minimal fluctuations in the execution times of the parallel
          threads (jobs) causes them to wait. Additionally it is often only one of
          the threads executing doing the actual reduce operation, which further
          impedes parallelism. A closer analysis of a couple of key algorithms used
          in science applications reveals that these global barriers are not always
          necessary. In many cases it is sufficient to synchronize a small subset
          of the threads. Any operation should proceed whenever the preconditions
          for its execution are met, and only those. Usually there is no need to
          wait for iterations of a loop to finish before you could continue calculating
          other things, all you need is to have those iterations done which were
          producing the required results for a particular next operation. Good bye
          global barriers, hello constraint based synchronization! People have been
          trying to build this type of computing (and even computers) already back
          in the 1970's. The theory behind what they did is based on ideas around
          static and dynamic dataflow. There are certain attempts today to get back
          to those ideas and to incorporate them with modern architectures. For instance,
          a lot of work is being done in the area of constructing dataflow oriented
          execution trees. Our results show that employing dataflow techniques in
          combination with the other ideas, as outlined herein, considerabley improves
          scalability for many problems.
        </p>
<h6>
<a name="hpx.tutorial.intro.principles.h3"></a>
          <span class="phrase"><a name="hpx.tutorial.intro.principles.locality_control"></a></span><a class="link" href="principles.html#hpx.tutorial.intro.principles.locality_control">Adaptive
          Locality Control instead of Static Data Distribution</a>
        </h6>
<p>
          While this principle seems to be a given for single desktop or laptop computers
          (the operating system is your friend), it is everything but ubiquitous
          on modern supercomputers, which are usually built from a large number of
          separate nodes (i.e. Beowulf clusters), tightly interconnected by a high
          bandwidth, low latency network. Today's prevalent programming model for
          those is <a href="http://en.wikipedia.org/wiki/Message_Passing_Interface" target="_top">MPI</a>
          which does not directly help with proper data distribution, leaving it
          to the programmer to decompose the data to all of the nodes the application
          is running on. There are a couple of specialized languages and programming
          environments based on <a href="http://www.pgas.org/" target="_top">PGAS</a> (Partitioned
          Global Address Space) designed to overcome this limitation, such as <a href="http://chapel.cray.com/" target="_top">Chapel</a>, <a href="http://x10-lang.org/" target="_top">X10</a>,
          <a href="http://upc.lbl.gov/" target="_top">UPC</a>, or <a href="http://labs.oracle.com/projects/plrg/Publications/index.html" target="_top">Fortress</a>.
          However all systems based on <a href="http://www.pgas.org/" target="_top">PGAS</a>
          rely on static data distribution. This works fine as long as such a static
          data distribution does not result in inhomogeneous workload distributions
          or other resource utilization imbalances. In a distributed system these
          imbalances can be mitigated by migrating part of the application data to
          different localities (nodes). The only framework supporting (limited) migration
          today is <a href="http://charm.cs.uiuc.edu/" target="_top">Charm++</a>. The first
          attempts towards solving related problem go back decades as well, a good
          example is the <a href="http://en.wikipedia.org/wiki/Linda_(coordination_language)" target="_top">Linda
          coordination language</a>. Nevertheless, none of the other mentioned
          systems support data migration today, which forces the users to either
          rely on static data distribution and live with the related performance
          hits or to implement everything themselves, which is very tedious and difficult.
          We believe that the only viable way to flexibly support dynamic and adaptive
          locality control is to provide a global, uniform address space to the applications,
          even on distributed systems.
        </p>
<h6>
<a name="hpx.tutorial.intro.principles.h4"></a>
          <span class="phrase"><a name="hpx.tutorial.intro.principles.move_work"></a></span><a class="link" href="principles.html#hpx.tutorial.intro.principles.move_work">Prefer
          Moving Work to the Data over Moving Data to the Work</a>
        </h6>
<p>
          For best performance it seems obvious to minimize the amount of bytes transferred
          from one part of the system to another. This is true on all levels. At
          the lowest level we try to take advantage of processor memory caches, thus
          minimizing memory latencies. Similarly, we try to amortize the data transfer
          time to and from <a href="http://en.wikipedia.org/wiki/GPGPU" target="_top">GPGPUs</a>
          as much as possible. At high levels we try to minimize data transfer between
          different nodes of a cluster or between different virtual machines on the
          cloud. Our experience (well, it's almost common wisdom) show that the amount
          of bytes necessary to encode a certain operation is very often much smaller
          than the amount of bytes encoding the data the operation is performed upon.
          Nevertheless we still often transfer the data to a particular place where
          we execute the operation just to bring the data back to where it came from
          afterwards. As an example let me look at the way we usually write our applications
          for clusters using <a href="http://en.wikipedia.org/wiki/Message_Passing_Interface" target="_top">MPI</a>.
          This programming model is all about data transfer between nodes. <a href="http://en.wikipedia.org/wiki/Message_Passing_Interface" target="_top">MPI</a>
          is the prevalent programming model for clusters, it is fairly straightforward
          to understand and to use. Therefore, we often write the applications in
          a way accommodating this model, centered around data transfer. These applications
          usually work well for smaller problem sizes and for regular data structures.
          The larger the amount of data we have to churn and the more irregular the
          problem domain becomes, the worse are the overall machine utilization and
          the (strong) scaling characteristics. While it is not impossible to implement
          more dynamic, data driven, and asynchronous applications using <a href="http://en.wikipedia.org/wiki/Message_Passing_Interface" target="_top">MPI</a>,
          it is overly difficult to so. At the same time, if we look at applications
          preferring to execute the code close the locality where the data was placed,
          i.e. utilizing active messages (for instance based on <a href="http://charm.cs.uiuc.edu/" target="_top">Charm++</a>),
          we see better asynchrony, simpler application codes, and improved scaling.
        </p>
<h6>
<a name="hpx.tutorial.intro.principles.h5"></a>
          <span class="phrase"><a name="hpx.tutorial.intro.principles.message_driven"></a></span><a class="link" href="principles.html#hpx.tutorial.intro.principles.message_driven">Favor
          Message Driven Computation over Message Passing</a>
        </h6>
<p>
          Today's prevalently used programming model on parallel (multi-node) systems
          is <a href="http://en.wikipedia.org/wiki/Message_Passing_Interface" target="_top">MPI</a>.
          It is based on message passing (as the name implies), which means that
          the receiver has to be aware of a message about to come in. Both codes,
          the sender and the receiver, have to synchronize in order to perform the
          communication step. Even the newer, asynchronous interfaces require to
          explicitly code the algorithms around the required communication scheme.
          As a result, any more than trivial <a href="http://en.wikipedia.org/wiki/Message_Passing_Interface" target="_top">MPI</a>
          application spends a considerable amount of time waiting for incoming messages,
          thus causing starvation and latencies to impede full resource utilization.
          The more complex and more dynamic the data structures and algorithms become,
          the larger are the adverse effects. The community has discovered message-driven
          and (data-driven) methods of implementing algorithms a long time ago, and
          systems such as <a href="http://charm.cs.uiuc.edu/" target="_top">Charm++</a>
          already have integrated active messages demonstrating the validity of the
          concept. Message driven computation allows to send messages without that
          the receiver has to actively wait for them. Any incoming message is handled
          asynchronously and triggers the encoded action by passing along arguments
          and - possibly - continuations. <span class="emphasis"><em>HPX</em></span> combines this
          scheme with work queue based scheduling as described above, which allows
          to almost completely overlap any communication with useful work, reducing
          latencies to a minimum.
        </p>
</div>
<table xmlns:rev="http://www.cs.rpi.edu/~gregod/boost/tools/doc/revision" width="100%"><tr>
<td align="left"></td>
<td align="right"><div class="copyright-footer">Copyright &#169; 2011-2014 The
      STE||AR Group, Louisiana State University<p>
        Distributed under the Boost Software License, Version 1.0. (See accompanying
        file <a href="http://www.boost.org/LICENSE_1_0.txt" target="_top">LICENSE_1_0.txt</a>
        or copy at <a href="http://www.boost.org/LICENSE_1_0.txt" target="_top">http://www.boost.org/LICENSE_1_0.txt</a>)
      </p>
</div></td>
</tr></table>
<hr>
<div class="spirit-nav">
<a accesskey="p" href="new_response.html"><img src="../../../images/prev.png" alt="Prev"></a><a accesskey="u" href="../intro.html"><img src="../../../images/up.png" alt="Up"></a><a accesskey="h" href="../../../index.html"><img src="../../../images/home.png" alt="Home"></a><a accesskey="n" href="../examples.html"><img src="../../../images/next.png" alt="Next"></a>
</div>
</body>
</html>
